seed: 42
base_model: models/Llama-2-7b-hf
data_path: data/train_medmcqa_alpaca_10k.jsonl
output_dir: runs/pipeline_med_iter
iterations: 2
warmup:
  sample_size: 100
  limit: 1000
  train:
    model_max_length: 512
    per_device_train_batch_size: 1
    num_train_epochs: 1
    learning_rate: 0.00002
    global_batch_size: 1
    mode: sft
score:
  max_samples: 200
select:
  nll_reject_ratio: 0.1
  select_ratio: 0.1
select_train:
  train:
    model_max_length: 512
    per_device_train_batch_size: 1
    num_train_epochs: 1
    learning_rate: 0.00002
    global_batch_size: 1
    mode: sft
